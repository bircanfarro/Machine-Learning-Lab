# Machine Learning Lab

This lab is aimed to walk you through the complete workflow of a machine learning project; from data wrangling, exploratory data analysis (EDA), model training and model evaluation/comparison. 

You will work with your machine project teamates for this lab and your team needs to decide whether to use either R or Python as the main programming language. The team leader needs to create a Github repository and invite the teammates as collaborators. Check how to do it [here](https://help.github.com/articles/inviting-collaborators-to-a-personal-repository/)

At the end of each part, we will do a **code review** withiin each group. Details will be shared later.


## Part I: Preprocessing and EDA

- The data comes from a global e-retailer company, including orders from 2012 to 2015. Import the **Orders** dataset and do some basic EDA. 
- For problem 1 to 3, we mainly focus on data cleaning and data visualizations. You can use all the packages that you are familiar with to conduct some plots and also provide **brief interpretations** about your findings.

### Problem 1: Dataset Import & Cleaning
Check **"Profit"** and **"Sales"** in the dataset, convert these two columns to numeric type. 

```{r}
library(dplyr)
#path = ".\data\Orders.csv"
path = "./data/Orders.csv"
df_orders <- read.csv(path, stringsAsFactors = F)

df_orders$Profit = as.numeric(gsub("[$,]", "", df_orders$Profit))
df_orders$Sales = as.numeric(gsub("[$,]", "", df_orders$Sales))
df_orders$Order.Date = as.Date(df_orders$Order.Date,"%m/%d/%y")

```



### Problem 2: Inventory Management
- Retailers that depend on seasonal shoppers have a particularly challenging job when it comes to inventory management. Your manager is making plans for next year's inventory.
- He wants you to answer the following questions:
    1. Is there any seasonal trend of inventory in the company?
    2. Is the seasonal trend the same for different categories?

- ***Hint:*** For each order, it has an attribute called `Quantity` that indicates the number of product in the order. If an order contains more than one product, there will be multiple observations of the same order.
```{r}

```

### Problem 3: Why did customers make returns?
- Your manager required you to give a brief report (**Plots + Interpretations**) on returned orders.

	1. How much profit did we lose due to returns each year?


	2. How many customer returned more than once? more than 5 times?


	3. Which regions are more likely to return orders?


	4. Which categories (sub-categories) of products are more likely to be returned?

- ***Hint:*** Merge the **Returns** dataframe with the **Orders** dataframe using `Order.ID`.

### Code Review

- Wrap up all the code you have in a either R or Python script under your name. i.e (`name_preprocess.py`) and push it to the lab repository.
- We will use a cool feature called ["Blame"](https://blog.github.com/2008-11-18-playing-the-blame-game/) on Github to review the code. Try to "blame" your teammates' code, just don't go nuts :)
- **Note:** Jupyter Notebook is 


## Part II: Machine Learning and Business Use Case

Now your manager has a basic understanding of why customers returned orders. Next, he wants you to use machine learning to predict which orders are most likely to be returned. In this part, you will generate several features based on our previous findings and your manager's requirements.

### Problem 4: Feature Engineering
#### Step 1: Create the dependent variable
- First of all, we need to generate a categorical variable which indicates whether an order has been returned or not.
- ***Hint:*** the returned orders’ IDs are contained in the dataset “returns”
```{r}
library(dplyr)
df_returns <- read.csv('./data/Returns.csv', stringsAsFactors = F)
df_orders_with_returns = merge(df_orders, select(df_returns, Order.ID,Returned), all.x= T)
df_orders_with_returns$Returned[is.na(df_orders_with_returns$Returned)] = "No"

```

#### Step 2:
- Your manager believes that **how long it took the order to ship** would affect whether the customer would return it or not. 
- He wants you to generate a feature which can measure how long it takes the company to process each order.
- ***Hint:*** Process.Time = Ship.Date - Order.Date
```{r}
df_process_time=mutate(df_orders_with_returns,Ship.Date = as.Date(Ship.Date,"%m/%d/%y"), Process.Time = Ship.Date - Order.Date)

#Double-check
View(select(df_process_time,Order.Date,Ship.Date, Process.Time))
```

#### Step 3:

- If a product has been returned before, it may be returned again. 
- Let us generate a feature that indicates how many times the product has been returned before.
- If it never got returned, we just impute using 0.
- ***Hint:*** Group by different Product.ID
```{r}
#product_return_counts = group_by(df_process_time,Product.ID) %>% 
  #tally(Returned=="Yes")
prc1 = group_by(df_process_time,Product.ID) %>% 
  count(m=Returned=="Yes") %>% 
  filter(m == TRUE)

returns_column = merge(df_process_time, prc1, by="Product.ID", all.x=T) %>% 
  select(-m)
returns_column$n[is.na(returns_column$n)] = 0
rename(returns_column,return_count = n)

#prc2= group_by(df_process_time,Product.ID) %>% 
  #summarise(Returned=="Yes")
```

### Problem 5: Fitting Models

- You can use any binary classification method you have learned so far.
- Use 80/20 training and test splits to build your model. 
- Double check the column types before you fit the model.
- Only include useful features. i.e all the `ID`s should be excluded from your training set.
- Not that there are only less than 5% of the orders have been returned, so you should consider using the `createDataPartition` function from `caret` package that does a **stratified** random split of the data.
- Do forget to `set.seed()` before the spilt to make your result reproducible.
- **Note:** We are not looking for the best tuned model in the lab so don't spend too much time on grid search. Focus on model evaluation and the business use case of each model.

```{r}
library(tree)
library(caret)
set.seed(0)
model_source = select(returns_column, -ends_with('ID'))
partition = createDataPartition(model_source$Returned,p= .2)
test_set = model_source[partition$Resample1,]
training_set = model_source[-partition$Resample1,]

classifier = tree(Returned ~ ., data=training_set)
plot(classifier)
#I have no idea why this tree does not seem to be doing anything useful.

```


### Problem 6: Evaluating Models
- What is the best metric to evaluate your model. Is accuracy good for this case?
- Now you have multiple models, which one would you pick? 
- Can you get any clue from the confusion matrix? What is the meaning of precision and recall in this case? Which one do you care the most? How will your model help the manager make decisions?
- **Note:** The last question is open-ended. Your answer could be completely different depending on your understanding of this business problem.

### Problem 7: Feature Engineering Revisit
- Is there anything wrong with the new feature we generated? How should we fix it?
- ***Hint***: For the real test set, we do not know it will get returned or not.
